{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wajahattt-dot/PIAIC-Q2-Notes-Wajahat-Hussain/blob/main/Class%2003%20Q2%20Batch%20%2061/PIAIC_Class_03_Q2_B61.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GUvuQP1zLV9D"
      },
      "source": [
        "Practice for the class of GenAI\n",
        "\n",
        "Topics:\n",
        "\n",
        "1. Installing Google Gemini in Google Colab (Python SDK)\n",
        "2.  Using images in Google Colab to get response from the Generatuve AI model\n",
        "3. Specifying History In Colab\n",
        "4. Specifying Tokens And Temperature To Maintain Efficiency"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AYIU7WSnAUmd"
      },
      "source": [
        "First we will install Google Generative AI model (Python SDK)\n",
        "to access the intelligence and develop in Python"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cshdOnw4sM7i"
      },
      "outputs": [],
      "source": [
        "!pip install -U -q \"google-generativeai>=0.7.2\" # Install the Python SDK"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Rq2Dzj3AjuD"
      },
      "source": [
        "Next we will import GenAI from Google"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VwV00xSG00cj"
      },
      "outputs": [],
      "source": [
        "import google.generativeai as genai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PUXYqfN8AuWw"
      },
      "source": [
        "We will provide access of the API key to use the Gemini Model\n",
        "\n",
        "We can generate new API key through Google AI Studio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "V4Dqf3bg05RK"
      },
      "outputs": [],
      "source": [
        "from google.colab import userdata\n",
        "GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')\n",
        "genai.configure(api_key=GOOGLE_API_KEY)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 957
        },
        "id": "qKoAn6Bn06V7",
        "outputId": "6b071bc9-10a8-4372-c370-ba4a4d251e01"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-2.0-flash-exp')\n",
        "response = model.generate_content(\"Give me python code to sort a list\")\n",
        "print(response)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFrC0J6v1DoQ",
        "outputId": "baece749-dc83-46ff-9357-7057bde1efe8"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "824"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "response.usage_metadata.candidates_token_count"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FCDnw2bHMAL8"
      },
      "source": [
        "## Use images in your prompt\n",
        "\n",
        "Here you will download an image from a URL and pass that image in our prompt.\n",
        "\n",
        "First, you download the image and load it with PIL:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rkgq8GOa1KFE",
        "outputId": "39c8de6e-542e-4ebf-b07e-98af3e9b4d82"
      },
      "outputs": [],
      "source": [
        "!curl -o image.jpg \"https://robbreport.com/wp-content/uploads/2021/07/1-15.jpg?w=1000\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vGdrCrmcMDTT"
      },
      "source": [
        "Then you open the image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "ppEm2SBT1Nf5",
        "outputId": "061d6998-2d41-4feb-c1c6-c7ba6c2ab5a3"
      },
      "outputs": [],
      "source": [
        "import PIL.Image\n",
        "img = PIL.Image.open('image.jpg')\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QFBcSiSKMHRh"
      },
      "source": [
        "Providing prompt for Gemini Chatbot Model 1.5 Flash"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "xq_tZUrn1SZs"
      },
      "outputs": [],
      "source": [
        "prompt = \"\"\"This image contains an image  of a potential product.\n",
        "Given the product image, describe the product as thoroughly as possible based on what you\n",
        "see in the image, making sure to note all of the product features. Return output in json format:\n",
        "{description: description, features: [feature1, feature2, feature3, etc]}\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hMNWQQFcMNiK"
      },
      "source": [
        "Generating the response by the help of AI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        },
        "id": "XykNPs1o1WUi",
        "outputId": "5fbb58ca-6cef-4359-c2e6-bad32adfafc5"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = model.generate_content([prompt, img])\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z9ZMfP_JMTUm"
      },
      "source": [
        "### Statleless Model\n",
        " Gemini Model Doesnt Memorize or Store Our Information Except within the prompt.\n",
        " For Instance:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "VXdu0z7k5AIA",
        "outputId": "9516f37c-5579-4d65-ec65-cc9b9ab43c48"
      },
      "outputs": [],
      "source": [
        "model= genai.GenerativeModel('gemini-1.5-flash')\n",
        "response = model.generate_content(\"My name is Abdul Qadir\")\n",
        "print(response.text)\n",
        "\n",
        "response = model.generate_content(\"My name is?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CDq3nhLIMvd4"
      },
      "source": [
        "Now we specify it to store our history so it can generate responses according to our background it knows about us"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "rpBu6pjQ3i-v"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "chat = model.start_chat(history=[])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JApt1KB9M61z"
      },
      "source": [
        "As it stores our information now, we can ask regarding to the background about which we asked from it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 86
        },
        "id": "BAH5KPDZ40GZ",
        "outputId": "a2222501-bba0-445d-b5fd-88fb3c491b65"
      },
      "outputs": [],
      "source": [
        "response = chat.send_message(\"Hi my name is Abdul Qadir\")\n",
        "print(response.text)\n",
        "response = chat.send_message(\"What is my name?\")\n",
        "print(response.text)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8L9jB6PkM4O_"
      },
      "source": [
        "We can see the chat history by"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5OngX6-s46He",
        "outputId": "67703ef6-def8-4ec5-cde5-cfa3990fe343"
      },
      "outputs": [],
      "source": [
        "print(chat.history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v-THRouuNKUI"
      },
      "source": [
        "## Specifying Tokens And Temperature\n",
        "Every prompt you send to the model includes parameters that control how the model generates responses. Use a `genai.GenerationConfig` to set these, or omit it to use the defaults.\n",
        "\n",
        "Temperature controls the degree of randomness in token selection. Use higher values for more creative responses, and lower values for more deterministic responses.\n",
        "\n",
        "You can set the `generation_config` when creating the model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "WkoWgsSG4-E1"
      },
      "outputs": [],
      "source": [
        "model = genai.GenerativeModel(\n",
        "    'gemini-1.5-flash',\n",
        "    generation_config=genai.GenerationConfig(\n",
        "        max_output_tokens=20,\n",
        "        temperature=0.9,\n",
        "    ))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WV7dl0oBNc-w"
      },
      "source": [
        "## Specifying the number of lines\n",
        "Or, set the `generation_config` on an individual call to `generate_content`. Any values set there override values on the model constructor.\n",
        "\n",
        "Note: Although you can set the `candidate_count` in the generation_config, gemini-pro models will only return a single candidate at the this time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "lj5NrizP-Ix9"
      },
      "outputs": [],
      "source": [
        "response = model.generate_content(\n",
        "    'Give me a numbered list of cat facts.',\n",
        "    # Limit to 5 facts.\n",
        "    generation_config = genai.GenerationConfig(stop_sequences=['\\n6'])\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Olx4M_Hn-P14",
        "outputId": "07df6200-4f8d-4672-cc09-85d57a62e839"
      },
      "outputs": [],
      "source": [
        "print(response.text)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyMro9kuycB2+CFXYOpZgTx7",
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
